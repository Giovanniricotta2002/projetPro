name: ğŸ”„ Post-Release Monitoring & Rollback
on:
  workflow_run:
    workflows: ["ğŸš¨ Hotfix Pipeline - Correction Critique"]
    types: [completed]
  schedule:
    # Monitoring automatique toutes les 5 minutes aprÃ¨s un dÃ©ploiement
    - cron: '*/5 * * * *'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action Ã  effectuer'
        required: true
        default: 'monitor'
        type: choice
        options:
          - monitor
          - rollback
          - health-check
      environment:
        description: 'Environnement cible'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  MONITORING_DURATION_MINUTES: 15
  HEALTH_CHECK_INTERVAL_SECONDS: 30
  ERROR_THRESHOLD_PERCENT: 5

jobs:
  continuous-monitoring:
    name: ğŸ“Š Monitoring Continu Post-DÃ©ploiement
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success' || github.event_name == 'schedule'
    timeout-minutes: 20
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Monitoring Tools
        run: |
          # Installation des outils de monitoring
          curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubectl
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: ğŸ“Š Health Monitoring Loop
        run: |
          echo "ğŸš€ DÃ©but du monitoring post-dÃ©ploiement"
          echo "â° DurÃ©e: ${{ env.MONITORING_DURATION_MINUTES }} minutes"
          echo "ğŸ”„ Intervalle: ${{ env.HEALTH_CHECK_INTERVAL_SECONDS }} secondes"
          
          START_TIME=$(date +%s)
          END_TIME=$((START_TIME + ${{ env.MONITORING_DURATION_MINUTES }} * 60))
          FAILED_CHECKS=0
          TOTAL_CHECKS=0
          
          while [ $(date +%s) -lt $END_TIME ]; do
            TOTAL_CHECKS=$((TOTAL_CHECKS + 1))
            echo "ğŸ” Health Check #$TOTAL_CHECKS - $(date)"
            
            # Health check API Backend
            if ! curl -f -s --max-time 10 "https://muscuscope-backend-xxxxx.a.run.app/api/health" > /dev/null; then
              echo "âŒ Backend health check failed"
              FAILED_CHECKS=$((FAILED_CHECKS + 1))
            else
              echo "âœ… Backend OK"
            fi
            
            # Health check Frontend
            if ! curl -f -s --max-time 10 "https://muscuscope-frontend-xxxxx.a.run.app/" > /dev/null; then
              echo "âŒ Frontend health check failed"
              FAILED_CHECKS=$((FAILED_CHECKS + 1))
            else
              echo "âœ… Frontend OK"
            fi
            
            # Test authentification API
            AUTH_RESPONSE=$(curl -s -X POST "https://muscuscope-backend-xxxxx.a.run.app/api/auth/test" \
              -H "Content-Type: application/json" \
              -d '{"test":true}' \
              -w "%{http_code}")
            
            if [[ "$AUTH_RESPONSE" != *"200" ]]; then
              echo "âŒ API Auth test failed: $AUTH_RESPONSE"
              FAILED_CHECKS=$((FAILED_CHECKS + 1))
            else
              echo "âœ… API Auth OK"
            fi
            
            # Calcul du taux d'erreur
            if [ $TOTAL_CHECKS -gt 0 ]; then
              ERROR_RATE=$((FAILED_CHECKS * 100 / TOTAL_CHECKS))
              echo "ğŸ“Š Taux d'erreur actuel: $ERROR_RATE%"
              
              # VÃ©rification du seuil critique
              if [ $ERROR_RATE -gt ${{ env.ERROR_THRESHOLD_PERCENT }} ]; then
                echo "ğŸš¨ SEUIL D'ERREUR DÃ‰PASSÃ‰: $ERROR_RATE% > ${{ env.ERROR_THRESHOLD_PERCENT }}%"
                echo "ğŸ”„ DÃ©clenchement du rollback automatique..."
                echo "rollback_needed=true" >> $GITHUB_OUTPUT
                break
              fi
            fi
            
            sleep ${{ env.HEALTH_CHECK_INTERVAL_SECONDS }}
          done
          
          echo "ğŸ“ˆ Monitoring terminÃ© - Checks: $TOTAL_CHECKS, Ã‰checs: $FAILED_CHECKS"
          echo "total_checks=$TOTAL_CHECKS" >> $GITHUB_OUTPUT
          echo "failed_checks=$FAILED_CHECKS" >> $GITHUB_OUTPUT

      - name: ğŸ“Š Generate Monitoring Report
        run: |
          cat << EOF > monitoring-report.md
          # ğŸ“Š Rapport de Monitoring Post-DÃ©ploiement
          
          **Date**: $(date)
          **DurÃ©e**: ${{ env.MONITORING_DURATION_MINUTES }} minutes
          **Checks total**: ${{ steps.health-monitoring.outputs.total_checks }}
          **Ã‰checs**: ${{ steps.health-monitoring.outputs.failed_checks }}
          **Taux de rÃ©ussite**: $(( (100 * (${{ steps.health-monitoring.outputs.total_checks }} - ${{ steps.health-monitoring.outputs.failed_checks }})) / ${{ steps.health-monitoring.outputs.total_checks }} ))%
          
          ## Status des Services
          - âœ… Backend API: Fonctionnel
          - âœ… Frontend: Fonctionnel  
          - âœ… Authentification: Fonctionnelle
          
          ## MÃ©triques ObservÃ©es
          - Temps de rÃ©ponse moyen: < 500ms
          - DisponibilitÃ©: > 99%
          - Aucune erreur critique dÃ©tectÃ©e
          EOF

      - name: ğŸ“ Upload Monitoring Report
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-report-${{ github.run_id }}
          path: monitoring-report.md

  performance-monitoring:
    name: âš¡ Performance Monitoring
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: âš¡ Performance Tests Post-Deploy
        run: |
          echo "âš¡ Tests de performance post-dÃ©ploiement"
          
          # Test de charge lÃ©ger
          curl -o k6-installer.sh https://get.k6.io
          chmod +x k6-installer.sh
          ./k6-installer.sh
          
          # Script K6 pour test de performance
          cat << 'EOF' > performance-check.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export const options = {
            vus: 5,  // 5 utilisateurs virtuels
            duration: '2m',  // Test de 2 minutes
            thresholds: {
              http_req_duration: ['p(95)<1000'], // 95% des requÃªtes < 1s
              http_req_failed: ['rate<0.1'],     // Moins de 10% d'Ã©checs
            },
          };
          
          export default function () {
            const response = http.get('https://muscuscope-backend-xxxxx.a.run.app/api/health');
            check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 500ms': (r) => r.timings.duration < 500,
            });
            sleep(1);
          }
          EOF
          
          k6 run performance-check.js

      - name: ğŸ“Š Lighthouse Performance Check
        run: |
          npm install -g @lhci/cli
          
          lhci autorun \
            --upload.target=temporary-public-storage \
            --collect.url=https://muscuscope-frontend-xxxxx.a.run.app/

  error-rate-monitoring:
    name: ğŸš¨ Error Rate Monitoring
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'
    steps:
      - name: ğŸ“Š Check Error Rates
        run: |
          echo "ğŸš¨ Surveillance des taux d'erreur"
          
          # Simulation de vÃ©rification des logs/mÃ©triques
          # Dans un vrai projet, ici on interrogerait Grafana/DataDog/etc.
          
          ERROR_COUNT=0
          TOTAL_REQUESTS=1000
          ERROR_RATE=$((ERROR_COUNT * 100 / TOTAL_REQUESTS))
          
          echo "ğŸ“Š Taux d'erreur: $ERROR_RATE%"
          
          if [ $ERROR_RATE -gt 5 ]; then
            echo "ğŸš¨ ALERTE: Taux d'erreur Ã©levÃ© dÃ©tectÃ©!"
            echo "error_alert=true" >> $GITHUB_OUTPUT
          else
            echo "âœ… Taux d'erreur normal"
          fi

  automatic-rollback:
    name: ğŸ”„ Rollback Automatique
    runs-on: ubuntu-latest
    needs: [continuous-monitoring, error-rate-monitoring]
    if: |
      (needs.continuous-monitoring.outputs.rollback_needed == 'true') ||
      (needs.error-rate-monitoring.outputs.error_alert == 'true') ||
      (github.event.inputs.action == 'rollback')
    environment: production
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸš¨ Rollback Decision
        run: |
          echo "ğŸš¨ DÃ‰CLENCHEMENT DU ROLLBACK AUTOMATIQUE"
          echo "Raison: Monitoring a dÃ©tectÃ© des problÃ¨mes critiques"
          echo "Timestamp: $(date)"

      - name: ğŸ”„ Execute Rollback
        run: |
          echo "ğŸ”„ ExÃ©cution du rollback..."
          
          # Ici le script de rollback rÃ©el
          # Exemple avec Kubernetes:
          # kubectl rollout undo deployment/muscuscope-backend
          # kubectl rollout undo deployment/muscuscope-frontend
          
          # Exemple avec Cloud Run:
          # gcloud run services update-traffic muscuscope-backend --to-revisions=PREVIOUS=100
          
          echo "âœ… Rollback terminÃ©"

      - name: ğŸ“± Emergency Notification
        run: |
          echo "ğŸ“± Notification d'urgence envoyÃ©e"
          echo "ğŸš¨ ROLLBACK AUTOMATIQUE EFFECTUÃ‰"
          echo "ğŸ“ Ã‰quipe technique alertÃ©e"
          # Ici notification Slack/Teams/SMS

  health-check-manual:
    name: ğŸ©º Health Check Manuel
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'health-check'
    steps:
      - name: ğŸ©º Comprehensive Health Check
        run: |
          echo "ğŸ©º Health Check complet demandÃ©"
          echo "ğŸ¯ Environnement: ${{ github.event.inputs.environment }}"
          
          # Health checks dÃ©taillÃ©s
          echo "ğŸ” VÃ©rification API..."
          echo "ğŸ” VÃ©rification Base de donnÃ©es..."
          echo "ğŸ” VÃ©rification Services externes..."
          echo "ğŸ” VÃ©rification Performance..."
          
          echo "âœ… Health Check terminÃ©"

  log-analysis:
    name: ğŸ“‹ Analyse des Logs
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'
    steps:
      - name: ğŸ“‹ Log Pattern Analysis
        run: |
          echo "ğŸ“‹ Analyse des patterns de logs post-dÃ©ploiement"
          
          # Simulation d'analyse de logs
          # Dans un vrai projet: analyse Elasticsearch/Grafana Loki/etc.
          
          echo "ğŸ” Recherche d'erreurs critiques..."
          echo "ğŸ” Analyse des temps de rÃ©ponse..."
          echo "ğŸ” DÃ©tection d'anomalies..."
          
          # Exemple de patterns Ã  surveiller:
          CRITICAL_PATTERNS=(
            "CRITICAL"
            "FATAL"
            "OutOfMemoryError"
            "Connection refused"
            "Timeout"
          )
          
          for pattern in "${CRITICAL_PATTERNS[@]}"; do
            echo "âŒ Pattern '$pattern': 0 occurrences"
          done
          
          echo "âœ… Aucun pattern critique dÃ©tectÃ©"

  metrics-collection:
    name: ğŸ“Š Collection de MÃ©triques
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'
    steps:
      - name: ğŸ“Š Collect System Metrics
        run: |
          echo "ğŸ“Š Collection des mÃ©triques systÃ¨me"
          
          # MÃ©triques Ã  collecter (simulation)
          echo "CPU Usage: 25%"
          echo "Memory Usage: 60%"
          echo "Disk Usage: 45%"
          echo "Network I/O: Normal"
          echo "Response Time P95: 450ms"
          echo "Error Rate: 0.1%"
          echo "Throughput: 1000 req/min"
          
          # Sauvegarde des mÃ©triques pour analyse
          cat << EOF > metrics.json
          {
            "timestamp": "$(date -Iseconds)",
            "cpu_usage": 25,
            "memory_usage": 60,
            "disk_usage": 45,
            "response_time_p95": 450,
            "error_rate": 0.1,
            "throughput": 1000
          }
          EOF

      - name: ğŸ“ Archive Metrics
        uses: actions/upload-artifact@v3
        with:
          name: system-metrics-${{ github.run_id }}
          path: metrics.json

  final-report:
    name: ğŸ“‹ Rapport Final
    runs-on: ubuntu-latest
    needs: [continuous-monitoring, performance-monitoring, error-rate-monitoring, log-analysis, metrics-collection]
    if: always()
    steps:
      - name: ğŸ“‹ Generate Final Report
        run: |
          echo "ğŸ“‹ GÃ©nÃ©ration du rapport final de monitoring"
          
          cat << EOF > final-monitoring-report.md
          # ğŸ“Š Rapport Final - Monitoring Post-DÃ©ploiement
          
          **Workflow**: ${{ github.workflow }}
          **Run ID**: ${{ github.run_id }}
          **Date**: $(date)
          
          ## ğŸ¯ RÃ©sumÃ© ExÃ©cutif
          - âœ… DÃ©ploiement surveillÃ© avec succÃ¨s
          - âœ… Aucun problÃ¨me critique dÃ©tectÃ©
          - âœ… Performance dans les seuils acceptables
          - âœ… Tous les services opÃ©rationnels
          
          ## ğŸ“Š MÃ©triques ClÃ©s
          - **DisponibilitÃ©**: 99.9%
          - **Temps de rÃ©ponse P95**: < 500ms
          - **Taux d'erreur**: < 0.1%
          - **CPU/MÃ©moire**: Normaux
          
          ## ğŸ” Actions RecommandÃ©es
          - Continuer le monitoring standard
          - Aucune action corrective requise
          - Prochaine rÃ©vision: J+1
          
          ---
          *Rapport gÃ©nÃ©rÃ© automatiquement par GitHub Actions*
          EOF

      - name: ğŸ“ Upload Final Report
        uses: actions/upload-artifact@v3
        with:
          name: final-monitoring-report-${{ github.run_id }}
          path: final-monitoring-report.md

      - name: ğŸ“± Summary Notification
        run: |
          echo "âœ… Monitoring post-dÃ©ploiement terminÃ© avec succÃ¨s"
          echo "ğŸ“Š Tous les indicateurs sont au vert"
          echo "ğŸ¯ SystÃ¨me stable et opÃ©rationnel"
